{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/travisstowe/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (24,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# pull starts raw data\n",
    "starts_df = pd.read_csv('../capstone_data/skillshare_2022_starts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter starts_df to our needed peramenters.\n",
    "# only annual\n",
    "starts_df = starts_df[starts_df['plan_length'] == 12]\n",
    "# not B2B\n",
    "starts_df = starts_df[starts_df['is_team'] == False]\n",
    "# no scholarships\n",
    "starts_df = starts_df[starts_df['is_scholarship'] == False]\n",
    "# has a free trial\n",
    "starts_df = starts_df[starts_df['is_direct_to_paid'] == False]\n",
    "starts_df = starts_df[starts_df['had_trial'] == True]\n",
    "# no special trial lengths\n",
    "starts_df = starts_df[starts_df['trial_length_offer'].isin(['One Week', 'One Month'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns have some potential value to prediction.\n",
    "prediction_cols = ['user_uid', 'create_time', 'first_payment_time', 'last_payment_attempt',\n",
    "                   'last_failed_payment_attempt', 'user_cancellation_time', 'cancellation_time', \n",
    "                   'refund_time', 'coupon_id', 'coupon_trial_length', 'payment_provider', 'payment_ux', \n",
    "                   'is_refunded', 'is_cancelled', 'has_paid', 'trial_end', 'first_payment_currency_code', 'original_trial_end', \n",
    "                   'extended_trial_end', 'was_trial_extended', 'is_trial_extension', 'is_split_trial', \n",
    "                   'trial_length_days', 'trial_length_offer', 'sub_utm_source', 'sub_utm_campaign', \n",
    "                   'sub_utm_medium', 'sub_utm_term', 'sub_utm_channel', 'referral_source', 'eligible_trial_number']\n",
    "\n",
    "clean_df = starts_df[prediction_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a successful conversion column.\n",
    "clean_df['success'] = 0\n",
    "\n",
    "# set to 1 if they paid\n",
    "clean_df['success'][clean_df['first_payment_time'].notnull()] = 1\n",
    "\n",
    "# return to 0 if they got a refund.\n",
    "clean_df['success'][clean_df['is_refunded']==1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create category code columns for each of the most relevant predictive columns\n",
    "clean_df['payment_provider_cat'] = clean_df['payment_provider'].astype('category')\n",
    "clean_df['payment_provider_cat_codes'] = clean_df['payment_provider_cat'].cat.codes\n",
    "\n",
    "clean_df['payment_ux_cat'] = clean_df['payment_ux'].astype('category')\n",
    "clean_df['payment_ux_cat_codes'] = clean_df['payment_ux_cat'].cat.codes\n",
    "\n",
    "clean_df['trial_length_offer_cat'] = clean_df['trial_length_offer'].astype('category')\n",
    "clean_df['trial_length_offer_cat_codes'] = clean_df['trial_length_offer_cat'].cat.codes\n",
    "\n",
    "clean_df['sub_utm_channel_cat'] = clean_df['sub_utm_channel'].astype('category')\n",
    "clean_df['sub_utm_channel_cat_codes'] = clean_df['sub_utm_channel_cat'].cat.codes\n",
    "\n",
    "clean_df['sub_utm_source_cat'] = clean_df['sub_utm_source'].astype('category')\n",
    "clean_df['sub_utm_source_cat_codes'] = clean_df['sub_utm_source_cat'].cat.codes\n",
    "\n",
    "clean_df['user_uid'] = clean_df['user_uid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all of the lookup columns for EDA analysis.\n",
    "payment_provider_lookup_df = clean_df.groupby(\n",
    "    by=['payment_provider', 'payment_provider_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "payment_provider_lookup_df.to_csv('lookup_payment_providers.csv')\n",
    "\n",
    "payment_ux_df = clean_df.groupby(\n",
    "    by=['payment_ux', 'payment_ux_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "payment_ux_df.to_csv('lookup_payment_ux.csv')\n",
    "\n",
    "trial_length_df = clean_df.groupby(\n",
    "    by=['trial_length_offer', 'trial_length_offer_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "trial_length_df.to_csv('lookup_trial_length_offer.csv')\n",
    "\n",
    "sub_utm_channel_df = clean_df.groupby(\n",
    "    by=['sub_utm_channel', 'sub_utm_channel_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "sub_utm_channel_df.to_csv('lookup_sub_utm_channel.csv')\n",
    "\n",
    "sub_utm_source_df = clean_df.groupby(\n",
    "    by=['sub_utm_source', 'sub_utm_source_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "sub_utm_source_df.to_csv('lookup_sub_utm_source.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of only the columms we want for prediction\n",
    "p_cols = ['user_uid', 'create_time', 'success', 'is_cancelled', 'payment_provider_cat_codes', 'payment_ux_cat_codes', \n",
    "          'trial_length_offer_cat_codes', 'sub_utm_channel_cat_codes', 'sub_utm_source_cat_codes']\n",
    "mlready_df = clean_df[p_cols]\n",
    "\n",
    "# remove duplicates by keeping the most recent subscription for any user_uid.\n",
    "mlready_df.sort_values(by='create_time', inplace=True)\n",
    "mlready_df = mlready_df.drop_duplicates(subset=['user_uid'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the video views data onto the starts.\n",
    "# creation of video views data file found in data_combine_v1.py\n",
    "vviews_df = pd.read_csv('../capstone_data/skillshare_2022_all_views.csv')\n",
    "\n",
    "# rename uid and remove unneeded columns.\n",
    "vviews_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "del vviews_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vviews to starts data and fill in missing data with 0.\n",
    "combo_df = mlready_df.merge(vviews_df, how='left', on='user_uid')\n",
    "combo_df = combo_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### begin to translate non-video activity to merge.\n",
    "\n",
    "# first we need to make sure non-video activity is during the trial for the user\n",
    "# so we need a dataframe of the the trial date range for the user.\n",
    "\n",
    "# make a day version of the trial start and end dates.\n",
    "starts_df['trial_end_day'] = pd.to_datetime(starts_df.original_trial_end).dt.date\n",
    "starts_df['trial_start_day'] = pd.to_datetime(starts_df.create_time).dt.date\n",
    "\n",
    "# for some reason user_uid is a float only on this dataframe. Change it to an int.\n",
    "starts_df['user_uid'] = starts_df['user_uid'].astype(int)\n",
    "\n",
    "# make a df for merging the trial start and end.\n",
    "trial_ends = starts_df[['user_uid', 'trial_start_day', 'trial_end_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Comments Data.\n",
    "# pull the comments data.\n",
    "comments_df = pd.read_csv('../capstone_data/skillshare_2022_comments.csv')\n",
    "\n",
    "# change user_id column name for easy merge.\n",
    "comments_df.rename(columns={'user_id':'user_uid'}, inplace=True)\n",
    "\n",
    "# merge on the trial start and end.\n",
    "comments_df = comments_df.merge(trial_ends, how='left', on='user_uid')\n",
    "\n",
    "# round create_time to created_day.\n",
    "comments_df['create_day'] = pd.to_datetime(comments_df.create_time).dt.date\n",
    "\n",
    "# filter data to comments that happened during the user's trial\n",
    "comments_df = comments_df[comments_df['create_day'] > comments_df['trial_start_day']]\n",
    "comments_df = comments_df[comments_df['create_day'] < comments_df['trial_end_day']]\n",
    "\n",
    "# make a groupby that for each user_uid that includes num of comments and total comment score.\n",
    "comment_gb_df = comments_df.groupby(by=['user_uid']).agg(\n",
    "    comment_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    comment_score=pd.NamedAgg(column='score', aggfunc='sum')).reset_index()\n",
    "\n",
    "# merge onto main dataframe.\n",
    "combo_df = combo_df.merge(comment_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Discussions Data.\n",
    "# Follow the same process as comments for discussions.\n",
    "discussions_df = pd.read_csv('../capstone_data/skillshare_2022_discussions.csv')\n",
    "discussions_df.rename(columns={'user_id':'user_uid'}, inplace=True)\n",
    "discussions_df = discussions_df.merge(trial_ends, how='left', on='user_uid')\n",
    "discussions_df['create_day'] = pd.to_datetime(discussions_df.create_time).dt.date\n",
    "discussions_df = discussions_df[discussions_df['create_day'] > discussions_df['trial_start_day']]\n",
    "discussions_df = discussions_df[discussions_df['create_day'] < discussions_df['trial_end_day']]\n",
    "discussions_gb_df = discussions_df.groupby(by=['user_uid']).agg(\n",
    "    discussion_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    discussion_score=pd.NamedAgg(column='score', aggfunc='sum')).reset_index()\n",
    "combo_df = combo_df.merge(discussions_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Follows Data.\n",
    "# Follow the same process as comments for follows.\n",
    "follows_df = pd.read_csv('../capstone_data/skillshare_2022_follows.csv')\n",
    "follows_df.rename(columns={'follower_uid':'user_uid'}, inplace=True)\n",
    "follows_df = follows_df.merge(trial_ends, how='left', on='user_uid')\n",
    "follows_df['follow_day'] = pd.to_datetime(follows_df.follow_time).dt.date\n",
    "follows_df = follows_df[follows_df['follow_day'] > follows_df['trial_start_day']]\n",
    "follows_df = follows_df[follows_df['follow_day'] < follows_df['trial_end_day']]\n",
    "\n",
    "# make a groupby by user_uid that counts the number of follows.\n",
    "follows_gb_df = follows_df.groupby(by=['user_uid']).agg(\n",
    "    follow_volume=pd.NamedAgg(column='target_uid', aggfunc='count')).reset_index()\n",
    "combo_df = combo_df.merge(follows_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Projects Data.\n",
    "# Follow the same process as comments for follows.\n",
    "projects_df = pd.read_csv('../capstone_data/skillshare_2022_projects.csv')\n",
    "projects_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "projects_df = projects_df.merge(trial_ends, how='left', on='user_uid')\n",
    "projects_df['create_day'] = pd.to_datetime(projects_df.create_time).dt.date\n",
    "projects_df = projects_df[projects_df['create_day'] > projects_df['trial_start_day']]\n",
    "projects_df = projects_df[projects_df['create_day'] < projects_df['trial_end_day']]\n",
    "projects_gb_df = projects_df.groupby(by=['user_uid']).agg(\n",
    "    projects_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    projects_score=pd.NamedAgg(column='num_up', aggfunc='sum')).reset_index()\n",
    "combo_df = combo_df.merge(projects_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Reviews Data.\n",
    "# Follow the same process as comments for follows.\n",
    "reviews_df = pd.read_csv('../capstone_data/skillshare_2022_reviews.csv')\n",
    "reviews_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "reviews_df = reviews_df.merge(trial_ends, how='left', on='user_uid')\n",
    "reviews_df['create_day'] = pd.to_datetime(reviews_df.create_time).dt.date\n",
    "reviews_df = reviews_df[reviews_df['create_day'] > reviews_df['trial_start_day']]\n",
    "reviews_df = reviews_df[reviews_df['create_day'] < reviews_df['trial_end_day']]\n",
    "\n",
    "# make a groupby for each user and their volume of reviews and avg review score.\n",
    "reviews_gb_df = reviews_df.groupby(by=['user_uid']).agg(\n",
    "    review_volume=pd.NamedAgg(column='review_id', aggfunc='count'), \n",
    "    rating_avg=pd.NamedAgg(column='rating', aggfunc='mean')).reset_index()\n",
    "combo_df = combo_df.merge(reviews_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "combo_df.to_csv('skillshare_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
