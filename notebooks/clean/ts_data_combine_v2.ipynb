{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/travisstowe/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (24,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# pull starts raw data\n",
    "starts_df = pd.read_csv('../capstone_data/skillshare_2022_starts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter starts_df to our needed peramenters.\n",
    "# only annual\n",
    "starts_df = starts_df[starts_df['plan_length'] == 12]\n",
    "# not B2B\n",
    "starts_df = starts_df[starts_df['is_team'] == False]\n",
    "# no scholarships\n",
    "starts_df = starts_df[starts_df['is_scholarship'] == False]\n",
    "# has a free trial\n",
    "starts_df = starts_df[starts_df['is_direct_to_paid'] == False]\n",
    "starts_df = starts_df[starts_df['had_trial'] == True]\n",
    "# remove updates; too complicated\n",
    "starts_df = starts_df[starts_df['was_upgraded'] == False]\n",
    "# no special trial lengths\n",
    "starts_df = starts_df[starts_df['trial_length_offer'].isin(['One Week', 'One Month'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason user_uid is a float only on this dataframe. Change it to an int.\n",
    "starts_df['user_uid'] = starts_df['user_uid'].astype(int)\n",
    "\n",
    "# make a day version of the trial start and end dates.\n",
    "starts_df['trial_end_day'] = pd.to_datetime(starts_df.original_trial_end)\n",
    "starts_df['trial_start_day'] = pd.to_datetime(starts_df.create_time)\n",
    "starts_df['cancellation_day'] = pd.to_datetime(starts_df.cancellation_time)\n",
    "starts_df['first_payment_day'] = pd.to_datetime(starts_df.first_payment_time)\n",
    "\n",
    "# Need to breakdown if the cancellation came before or after the first payment.\n",
    "starts_df['is_cancel_during_trial'] = 0\n",
    "starts_df['is_cancel_during_trial'][starts_df['cancellation_day'] <= starts_df['trial_end_day']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the columns have some potential value to prediction.\n",
    "prediction_cols = ['user_uid', 'create_time', 'first_payment_time', 'last_payment_attempt', \n",
    "                   'is_cancel_during_trial', 'trial_end_day', 'trial_start_day',\n",
    "                   'last_failed_payment_attempt', 'user_cancellation_time', 'cancellation_time', \n",
    "                   'refund_time', 'coupon_id', 'coupon_trial_length', 'payment_provider', 'payment_ux', \n",
    "                   'is_refunded', 'is_cancelled', 'has_paid', 'trial_end', 'first_payment_currency_code', 'original_trial_end', \n",
    "                   'extended_trial_end', 'was_trial_extended', 'is_trial_extension', 'is_split_trial', \n",
    "                   'trial_length_days', 'trial_length_offer', 'sub_utm_source', 'sub_utm_campaign', \n",
    "                   'sub_utm_medium', 'sub_utm_term', 'sub_utm_channel', 'referral_source', 'eligible_trial_number']\n",
    "\n",
    "clean_df = starts_df[prediction_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a successful conversion column.\n",
    "clean_df['success'] = 0\n",
    "\n",
    "# set to 1 if they paid\n",
    "clean_df['success'][clean_df['first_payment_time'].notnull()] = 1\n",
    "\n",
    "# return to 0 if they got a refund.\n",
    "clean_df['success'][clean_df['is_refunded']==1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create category code columns for each of the most relevant predictive columns\n",
    "clean_df['payment_provider_cat'] = clean_df['payment_provider'].astype('category')\n",
    "clean_df['payment_provider_cat_codes'] = clean_df['payment_provider_cat'].cat.codes\n",
    "\n",
    "clean_df['payment_ux_cat'] = clean_df['payment_ux'].astype('category')\n",
    "clean_df['payment_ux_cat_codes'] = clean_df['payment_ux_cat'].cat.codes\n",
    "\n",
    "clean_df['trial_length_offer_cat'] = clean_df['trial_length_offer'].astype('category')\n",
    "clean_df['trial_length_offer_cat_codes'] = clean_df['trial_length_offer_cat'].cat.codes\n",
    "\n",
    "clean_df['sub_utm_channel_cat'] = clean_df['sub_utm_channel'].astype('category')\n",
    "clean_df['sub_utm_channel_cat_codes'] = clean_df['sub_utm_channel_cat'].cat.codes\n",
    "\n",
    "clean_df['sub_utm_source_cat'] = clean_df['sub_utm_source'].astype('category')\n",
    "clean_df['sub_utm_source_cat_codes'] = clean_df['sub_utm_source_cat'].cat.codes\n",
    "\n",
    "clean_df['user_uid'] = clean_df['user_uid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all of the lookup columns for EDA analysis.\n",
    "payment_provider_lookup_df = clean_df.groupby(\n",
    "    by=['payment_provider', 'payment_provider_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "payment_provider_lookup_df.to_csv('lookup_payment_providers.csv')\n",
    "\n",
    "payment_ux_df = clean_df.groupby(\n",
    "    by=['payment_ux', 'payment_ux_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "payment_ux_df.to_csv('lookup_payment_ux.csv')\n",
    "\n",
    "trial_length_df = clean_df.groupby(\n",
    "    by=['trial_length_offer', 'trial_length_offer_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "trial_length_df.to_csv('lookup_trial_length_offer.csv')\n",
    "\n",
    "sub_utm_channel_df = clean_df.groupby(\n",
    "    by=['sub_utm_channel', 'sub_utm_channel_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "sub_utm_channel_df.to_csv('lookup_sub_utm_channel.csv')\n",
    "\n",
    "sub_utm_source_df = clean_df.groupby(\n",
    "    by=['sub_utm_source', 'sub_utm_source_cat_codes']).agg(\n",
    "        volume=pd.NamedAgg(column='user_uid', aggfunc='count')).reset_index()\n",
    "sub_utm_source_df.to_csv('lookup_sub_utm_source.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of only the columms we want for prediction\n",
    "p_cols = ['user_uid', 'create_time', 'success', 'cancellation_time', 'payment_provider_cat_codes', \n",
    "          'payment_ux_cat_codes', 'trial_length_offer_cat_codes', 'sub_utm_channel_cat_codes', \n",
    "          'sub_utm_source_cat_codes', 'trial_start_day', 'trial_end_day', 'is_cancel_during_trial']\n",
    "mlready_df = clean_df[p_cols]\n",
    "\n",
    "# remove duplicates by keeping the most recent subscription for any user_uid.\n",
    "mlready_df.sort_values(by='create_time', inplace=True)\n",
    "mlready_df = mlready_df.drop_duplicates(subset=['user_uid'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the video views data onto the starts.\n",
    "# creation of video views data file found in data_combine_v1.py\n",
    "vviews_df = pd.read_csv('../capstone_data/skillshare_2022_all_views.csv')\n",
    "\n",
    "# rename uid and remove unneeded columns.\n",
    "vviews_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "del vviews_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vviews to starts data and fill in missing data with 0.\n",
    "combo_df = mlready_df.merge(vviews_df, how='left', on='user_uid')\n",
    "combo_df = combo_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### begin to translate non-video activity to merge.\n",
    "\n",
    "# We need to make sure non-video activity is during the trial for the user\n",
    "# so we need a dataframe of the the trial date range for the user.\n",
    "trial_ends = starts_df[['user_uid', 'trial_start_day', 'trial_end_day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Comments Data.\n",
    "# pull the comments data.\n",
    "comments_df = pd.read_csv('../capstone_data/skillshare_2022_comments.csv')\n",
    "\n",
    "# change user_id column name for easy merge.\n",
    "comments_df.rename(columns={'user_id':'user_uid'}, inplace=True)\n",
    "\n",
    "# merge on the trial start and end.\n",
    "comments_df = comments_df.merge(trial_ends, how='left', on='user_uid')\n",
    "\n",
    "# round create_time to created_day.\n",
    "comments_df['create_day'] = pd.to_datetime(comments_df.create_time).dt.date\n",
    "\n",
    "# filter data to comments that happened during the user's trial\n",
    "comments_df = comments_df[comments_df['create_day'] > comments_df['trial_start_day']]\n",
    "comments_df = comments_df[comments_df['create_day'] < comments_df['trial_end_day']]\n",
    "\n",
    "# make a groupby that for each user_uid that includes num of comments and total comment score.\n",
    "comment_gb_df = comments_df.groupby(by=['user_uid']).agg(\n",
    "    comment_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    comment_score=pd.NamedAgg(column='score', aggfunc='sum')).reset_index()\n",
    "\n",
    "# merge onto main dataframe.\n",
    "combo_df = combo_df.merge(comment_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Discussions Data.\n",
    "# Follow the same process as comments for discussions.\n",
    "discussions_df = pd.read_csv('../capstone_data/skillshare_2022_discussions.csv')\n",
    "discussions_df.rename(columns={'user_id':'user_uid'}, inplace=True)\n",
    "discussions_df = discussions_df.merge(trial_ends, how='left', on='user_uid')\n",
    "discussions_df['create_day'] = pd.to_datetime(discussions_df.create_time)\n",
    "discussions_df = discussions_df[discussions_df['create_day'] > discussions_df['trial_start_day']]\n",
    "discussions_df = discussions_df[discussions_df['create_day'] < discussions_df['trial_end_day']]\n",
    "discussions_gb_df = discussions_df.groupby(by=['user_uid']).agg(\n",
    "    discussion_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    discussion_score=pd.NamedAgg(column='score', aggfunc='sum')).reset_index()\n",
    "combo_df = combo_df.merge(discussions_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Follows Data.\n",
    "# Follow the same process as comments for follows.\n",
    "follows_df = pd.read_csv('../capstone_data/skillshare_2022_follows.csv')\n",
    "follows_df.rename(columns={'follower_uid':'user_uid'}, inplace=True)\n",
    "follows_df = follows_df.merge(trial_ends, how='left', on='user_uid')\n",
    "follows_df['follow_day'] = pd.to_datetime(follows_df.follow_time)\n",
    "follows_df = follows_df[follows_df['follow_day'] > follows_df['trial_start_day']]\n",
    "follows_df = follows_df[follows_df['follow_day'] < follows_df['trial_end_day']]\n",
    "\n",
    "# make a groupby by user_uid that counts the number of follows.\n",
    "follows_gb_df = follows_df.groupby(by=['user_uid']).agg(\n",
    "    follow_volume=pd.NamedAgg(column='target_uid', aggfunc='count')).reset_index()\n",
    "combo_df = combo_df.merge(follows_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Projects Data.\n",
    "# Follow the same process as comments for follows.\n",
    "projects_df = pd.read_csv('../capstone_data/skillshare_2022_projects.csv')\n",
    "projects_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "projects_df = projects_df.merge(trial_ends, how='left', on='user_uid')\n",
    "projects_df['create_day'] = pd.to_datetime(projects_df.create_time)\n",
    "projects_df = projects_df[projects_df['create_day'] > projects_df['trial_start_day']]\n",
    "projects_df = projects_df[projects_df['create_day'] < projects_df['trial_end_day']]\n",
    "projects_gb_df = projects_df.groupby(by=['user_uid']).agg(\n",
    "    projects_volume=pd.NamedAgg(column='id', aggfunc='count'), \n",
    "    projects_score=pd.NamedAgg(column='num_up', aggfunc='sum')).reset_index()\n",
    "combo_df = combo_df.merge(projects_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge on Reviews Data.\n",
    "# Follow the same process as comments for follows.\n",
    "reviews_df = pd.read_csv('../capstone_data/skillshare_2022_reviews.csv')\n",
    "reviews_df.rename(columns={'uid':'user_uid'}, inplace=True)\n",
    "reviews_df = reviews_df.merge(trial_ends, how='left', on='user_uid')\n",
    "reviews_df['create_day'] = pd.to_datetime(reviews_df.create_time)\n",
    "reviews_df = reviews_df[reviews_df['create_day'] > reviews_df['trial_start_day']]\n",
    "reviews_df = reviews_df[reviews_df['create_day'] < reviews_df['trial_end_day']]\n",
    "\n",
    "# make a groupby for each user and their volume of reviews and avg review score.\n",
    "reviews_gb_df = reviews_df.groupby(by=['user_uid']).agg(\n",
    "    review_volume=pd.NamedAgg(column='review_id', aggfunc='count'), \n",
    "    rating_avg=pd.NamedAgg(column='rating', aggfunc='mean')).reset_index()\n",
    "combo_df = combo_df.merge(reviews_gb_df, how='left', on='user_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "combo_df.to_csv('skillshare_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {'total starts': len(combo_df),\n",
    "              'overall_p1nr': combo_df.success.sum() / len(combo_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_df = combo_df[combo_df['is_cancel_during_trial']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict['cancelled'] = len(cancelled_df)\n",
    "output_dict['cancelled_p1nr'] = cancelled_df.success.sum() / len(cancelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total starts': 473932,\n",
       " 'overall_p1nr': 0.2396546339981263,\n",
       " 'cancelled': 195213,\n",
       " 'cancelled_p1nr': 0.01041426544338748}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_df = combo_df[combo_df['is_cancel_during_trial']==0]\n",
    "output_dict['not_cancelled'] = len(not_cancelled_df)\n",
    "output_dict['not_cancelled_p1nr'] = not_cancelled_df.success.sum() / len(not_cancelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total starts': 473932,\n",
       " 'overall_p1nr': 0.2396546339981263,\n",
       " 'cancelled': 195213,\n",
       " 'cancelled_p1nr': 0.01041426544338748,\n",
       " 'not_cancelled': 278719,\n",
       " 'not_cancelled_p1nr': 0.4002131178714045}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_df = not_cancelled_df.fillna(0.0)\n",
    "not_cancelled_df['nonV_engagement'] = not_cancelled_df['comment_volume'] + not_cancelled_df['discussion_volume'] + not_cancelled_df['follow_volume'] + not_cancelled_df['projects_volume'] + not_cancelled_df['review_volume']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_df['has_nonV_engagement'] = 0\n",
    "not_cancelled_df['has_nonV_engagement'][not_cancelled_df['nonV_engagement'] > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_nonve_df = not_cancelled_df[not_cancelled_df['has_nonV_engagement']==0]\n",
    "output_dict['no_nonV'] = len(not_cancelled_nonve_df)\n",
    "output_dict['no_nonV_p1nr'] = not_cancelled_nonve_df.success.sum() / len(not_cancelled_nonve_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total starts': 473932,\n",
       " 'overall_p1nr': 0.2396546339981263,\n",
       " 'cancelled': 195213,\n",
       " 'cancelled_p1nr': 0.01041426544338748,\n",
       " 'not_cancelled': 278719,\n",
       " 'not_cancelled_p1nr': 0.4002131178714045,\n",
       " 'no_nonV': 217346,\n",
       " 'no_nonV_p1nr': 0.3902303239995215}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_nve_df = not_cancelled_df[not_cancelled_df['has_nonV_engagement']==1]\n",
    "output_dict['nonV'] = len(not_cancelled_nve_df)\n",
    "output_dict['nonV_p1nr'] = not_cancelled_nve_df.success.sum() / len(not_cancelled_nve_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total starts': 473932,\n",
       " 'overall_p1nr': 0.2396546339981263,\n",
       " 'cancelled': 195213,\n",
       " 'cancelled_p1nr': 0.01041426544338748,\n",
       " 'not_cancelled': 278719,\n",
       " 'not_cancelled_p1nr': 0.4002131178714045,\n",
       " 'no_nonV': 217346,\n",
       " 'no_nonV_p1nr': 0.3902303239995215,\n",
       " 'nonV': 61373,\n",
       " 'nonV_p1nr': 0.4355661284278103}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cancelled_df['totalmin-day-3'] = not_cancelled_df['day-1'] + not_cancelled_df['day-2'] + not_cancelled_df['day-3']\n",
    "not_cancelled_df['totalmin-day-7'] = not_cancelled_df['totalmin-day-3'] + not_cancelled_df['day-4'] + not_cancelled_df['day-5'] + not_cancelled_df['day-6'] + not_cancelled_df['day-7']\n",
    "not_cancelled_df['totalmin-day-14'] = not_cancelled_df['totalmin-day-7'] + not_cancelled_df['day-8'] + not_cancelled_df['day-9'] + not_cancelled_df['day-10'] + not_cancelled_df['day-11'] +  + not_cancelled_df['day-12'] + not_cancelled_df['day-13'] + not_cancelled_df['day-14']\n",
    "not_cancelled_df['totalmin'] = not_cancelled_df['totalmin-day-14']\n",
    "for x in range(15, 31):\n",
    "    daystr = 'day-' + str(x)\n",
    "    not_cancelled_df['totalmin'] = not_cancelled_df['totalmin'] + not_cancelled_df[daystr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: 0.0\n",
      "Below: 0.302      Above: 0.426\n",
      "----------------------------\n",
      " \n",
      "Checking: 10.0\n",
      "Below: 0.312      Above: 0.427\n",
      "----------------------------\n",
      " \n",
      "Checking: 20.0\n",
      "Below: 0.314      Above: 0.429\n",
      "----------------------------\n",
      " \n",
      "Checking: 30.0\n",
      "Below: 0.316      Above: 0.430\n",
      "----------------------------\n",
      " \n",
      "Checking: 40.0\n",
      "Below: 0.317      Above: 0.431\n",
      "----------------------------\n",
      " \n",
      "Checking: 50.0\n",
      "Below: 0.318      Above: 0.432\n",
      "----------------------------\n",
      " \n",
      "Checking: 60.0\n",
      "Below: 0.320      Above: 0.432\n",
      "----------------------------\n",
      " \n",
      "Checking: 70.0\n",
      "Below: 0.321      Above: 0.433\n",
      "----------------------------\n",
      " \n",
      "Checking: 80.0\n",
      "Below: 0.321      Above: 0.434\n",
      "----------------------------\n",
      " \n",
      "Checking: 90.0\n",
      "Below: 0.322      Above: 0.434\n",
      "----------------------------\n",
      " \n",
      "Checking: 100.0\n",
      "Below: 0.323      Above: 0.435\n",
      "----------------------------\n",
      " \n",
      "Checking: 110.0\n",
      "Below: 0.324      Above: 0.435\n",
      "----------------------------\n",
      " \n",
      "Checking: 120.0\n",
      "Below: 0.325      Above: 0.435\n",
      "----------------------------\n",
      " \n",
      "Checking: 130.0\n",
      "Below: 0.325      Above: 0.436\n",
      "----------------------------\n",
      " \n",
      "Checking: 140.0\n",
      "Below: 0.326      Above: 0.436\n",
      "----------------------------\n",
      " \n",
      "Checking: 150.0\n",
      "Below: 0.327      Above: 0.437\n",
      "----------------------------\n",
      " \n",
      "Checking: 160.0\n",
      "Below: 0.327      Above: 0.437\n",
      "----------------------------\n",
      " \n",
      "Checking: 170.0\n",
      "Below: 0.328      Above: 0.437\n",
      "----------------------------\n",
      " \n",
      "Checking: 180.0\n",
      "Below: 0.328      Above: 0.438\n",
      "----------------------------\n",
      " \n",
      "Checking: 190.0\n",
      "Below: 0.329      Above: 0.438\n",
      "----------------------------\n",
      " \n",
      "Checking: 200.0\n",
      "Below: 0.329      Above: 0.438\n",
      "----------------------------\n",
      " \n",
      "Checking: 210.0\n",
      "Below: 0.329      Above: 0.439\n",
      "----------------------------\n",
      " \n",
      "Checking: 220.0\n",
      "Below: 0.330      Above: 0.439\n",
      "----------------------------\n",
      " \n",
      "Checking: 230.0\n",
      "Below: 0.330      Above: 0.439\n",
      "----------------------------\n",
      " \n",
      "Checking: 240.0\n",
      "Below: 0.330      Above: 0.440\n",
      "----------------------------\n",
      " \n",
      "Checking: 250.0\n",
      "Below: 0.331      Above: 0.440\n",
      "----------------------------\n",
      " \n",
      "Checking: 260.0\n",
      "Below: 0.331      Above: 0.440\n",
      "----------------------------\n",
      " \n",
      "Checking: 270.0\n",
      "Below: 0.331      Above: 0.441\n",
      "----------------------------\n",
      " \n",
      "Checking: 280.0\n",
      "Below: 0.332      Above: 0.441\n",
      "----------------------------\n",
      " \n",
      "Checking: 290.0\n",
      "Below: 0.332      Above: 0.441\n",
      "----------------------------\n",
      " \n",
      "Checking: 300.0\n",
      "Below: 0.332      Above: 0.441\n",
      "----------------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for a in range(31):\n",
    "    x = a * 10.0\n",
    "    print('Checking:', x)\n",
    "    check_bad = not_cancelled_df[not_cancelled_df['totalmin-day-14']<=x]\n",
    "    check_good = not_cancelled_df[not_cancelled_df['totalmin-day-14']>x]\n",
    "    print('Below:', \"{0:0.3f}\".format(check_bad.success.sum() / len(check_bad)), '     Above:', \"{0:0.3f}\".format(check_good.success.sum() / len(check_good)))\n",
    "    print('----------------------------')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4441627204593758"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_cancelled_nve_df = check_good[check_good['has_nonV_engagement']==1]\n",
    "not_cancelled_nve_df.success.sum() / len(not_cancelled_nve_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_cancelled_df['mw_in_3'] = not_cancelled_df['day-1']\n",
    "# not_cancelled_df['mw_in_3_above_15'] = 0\n",
    "# not_cancelled_df['mw_in_3_above_15'][not_cancelled_df['mw_in_3'] > 3600] = 1\n",
    "# above_15 = not_cancelled_df[not_cancelled_df['mw_in_3_above_15'] == 1]\n",
    "# below_15 = not_cancelled_df[not_cancelled_df['mw_in_3_above_15'] == 0]\n",
    "# print('above 15 Rate:', len(above_15[above_15['success']==1]) / len(above_15))\n",
    "# print('below 15 Rate:', len(below_15[below_15['success']==1]) / len(below_15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
